params={'frame_shift_ms': 10.0, 'allowed_excess_duration_ratio': 0.1, 'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 50, 'reset_interval': 200, 'valid_interval': 3000, 'feature_dim': 80, 'subsampling_factor': 4, 'warm_step': 2000, 'env_info': {'k2-version': '1.23.4', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': '2700d9558611e33b1da13608121e23a49bc69701', 'k2-git-date': 'Mon May 22 10:00:23 2023', 'lhotse-version': '1.13.0', 'torch-version': '1.12.1+cu113', 'torch-cuda-available': True, 'torch-cuda-version': '11.3', 'python-version': '3.1', 'icefall-git-branch': 'pruned-distillation', 'icefall-git-sha1': '9942098-dirty', 'icefall-git-date': 'Tue Feb 20 12:15:37 2024', 'icefall-path': '/home/sskim/miniconda3/lib/python3.10/site-packages/icefall-1.0-py3.10.egg', 'k2-path': '/home/sskim/miniconda3/lib/python3.10/site-packages/k2-1.23.4.dev20240124+cuda11.3.torch1.12.1-py3.10-linux-x86_64.egg/k2/__init__.py', 'lhotse-path': '/home/sskim/miniconda3/lib/python3.10/site-packages/lhotse/__init__.py', 'hostname': 'docker', 'IP address': '172.17.0.2'}, 'vocab_size': 500, 'blank_id': 0, 'context_size': 2, 'num_encoder_layers': '2,4,3,2,4', 'feedforward_dims': '1024,1024,2048,2048,1024', 'nhead': '8,8,8,8,8', 'encoder_dims': '384,384,384,384,384', 'attention_dims': '192,192,192,192,192', 'encoder_unmasked_dims': '256,256,256,256,256', 'zipformer_downsampling_factors': '1,2,4,8,2', 'cnn_module_kernels': '31,31,31,31,31', 'decoder_dim': 512, 'joiner_dim': 512}
device=device(type='cuda', index=0)
Number of model parameters: 70561891
y=RaggedTensor([[29, 32, 69, 0, 77, 64, 20, 21, 14, 74]], dtype=torch.int32)
alphas=tensor([[[   0.0000,   -6.1522,  -12.3048,  -18.4539,  -24.6061,  -30.7599,
           -36.9145,  -43.0653,  -49.2194,  -55.3740,  -61.5288,  -67.6799,
           -73.8342,  -79.9894,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [  -6.3697,  -11.9222,  -17.7578,  -23.7061,  -29.7161,  -35.7688,
           -41.8462,  -47.9389,  -54.0449,  -60.1622,  -66.2864,  -72.4141,
           -78.5488,  -84.8985,  -91.2473,  -97.5941, -103.9430, -110.2928,
          -116.6418, -122.9884, -129.3369, -135.6910, -142.0444, -148.3953,
          -154.7482,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [ -13.0037,  -18.1121,  -23.6261,  -29.3153,  -35.1134,  -40.9809,
           -46.8956,  -52.8408,  -58.8137,  -64.8065,  -70.8164,  -76.8367,
           -82.8715,  -88.9348,  -95.0165, -101.1090, -107.2137, -113.3269,
          -119.4456, -125.5661, -131.6927, -137.8257, -143.9607, -150.0947,
          -156.2382, -162.3795,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [ -19.2347,  -24.0545,  -29.3435,  -34.8496,  -40.4904,  -46.2242,
           -52.0219,  -57.8634,  -63.7467,  -69.6600,  -75.5961,  -81.5483,
           -87.5217,  -93.5137,  -99.5219, -105.5417, -111.5774, -117.6265,
          -123.6850, -129.7487, -135.8229, -141.9075, -147.9975, -154.0892,
          -160.1887, -166.3939, -172.5988, -178.8005, -185.0047, -191.2111,
          -197.4173,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [ -25.4366,  -30.0067,  -35.0852,  -40.4120,  -45.8952,  -51.4842,
           -57.1491,  -62.8690,  -68.6380,  -74.4437,  -80.2793,  -86.1367,
           -92.0187,  -97.9215, -103.8412, -109.7730, -115.7216, -121.6828,
          -127.6552, -133.6339, -139.6246, -145.6251, -151.6333, -157.6443,
          -163.6638, -169.6918, -175.7294, -181.7705, -187.8204, -193.8766,
          -199.9632, -206.0460, -212.1316, -218.2143, -224.2979, -230.3779,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [ -32.1606,  -36.7339,  -41.8136,  -47.1419,  -52.6116,  -58.2037,
           -63.8698,  -69.5919,  -75.3585,  -81.1674,  -87.0041,  -92.8631,
           -98.7317, -104.6377, -110.0525, -115.7458, -121.5559, -127.4269,
          -133.3347, -139.2656, -145.2120, -151.1797, -157.1600, -163.1481,
          -169.1476, -175.1534, -181.1679, -187.1882, -193.2159, -199.2544,
          -205.3051, -211.3622, -217.4288, -223.4972, -229.5690, -235.9026,
          -242.2390, -248.5777, -254.9155, -261.2508, -267.5886, -273.9324,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf, -111.1063, -116.5219, -122.2114, -128.0181, -133.8862,
          -139.7950, -145.7220, -151.6896, -157.6545, -163.6358, -169.6201,
          -175.6162, -180.9915, -186.6581, -192.4402, -198.2934, -204.1907,
          -210.1198, -216.0688, -222.0404, -228.0235, -234.0196, -240.0511,
          -246.1111, -252.1904, -258.2833, -264.3827, -270.4924, -276.6347,
          -282.7775, -288.9172, -295.0596, -301.2071, -307.3553, -313.5002,
          -319.6473],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
          -181.7981, -187.1755, -192.3928, -197.9278, -203.6075, -209.3755,
          -215.2007, -221.0621, -226.9585, -232.8793, -238.8198, -244.7792,
          -250.7644, -256.7698, -262.7921, -268.8254, -274.8752, -280.9459,
          -287.0273, -293.1139, -299.2108, -305.3179, -311.4301, -317.5430,
          -323.7829],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf, -193.0679, -198.2901, -203.8219, -209.5072, -215.2721,
          -221.1023, -226.4422, -232.0804, -237.8350, -243.6597, -249.5256,
          -255.4283, -261.3594, -267.3157, -273.2877, -279.2820, -285.2964,
          -291.3257, -297.3632, -303.4138, -309.4764, -315.5486, -321.6243,
          -327.8795],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
          -227.3243, -232.6623, -238.3008, -244.0516, -249.8768, -255.7409,
          -261.0763, -266.7046, -272.4574, -278.2776, -284.1507, -290.0634,
          -296.0017, -301.9580, -307.9352, -313.9305, -319.9380, -325.9536,
          -332.1356],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf, -261.9233,
          -267.2597, -272.8819, -278.6358, -284.4529, -290.3436, -296.2502,
          -301.6283, -307.2917, -313.0849, -318.9457, -324.8493, -330.7795,
          -337.0060]]], device='cuda:0',
       grad_fn=<MutualInformationRecursionFunctionBackward>)
alphas.detach()=tensor([[[   0.0000,   -6.1522,  -12.3048,  -18.4539,  -24.6061,  -30.7599,
           -36.9145,  -43.0653,  -49.2194,  -55.3740,  -61.5288,  -67.6799,
           -73.8342,  -79.9894,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [  -6.3697,  -11.9222,  -17.7578,  -23.7061,  -29.7161,  -35.7688,
           -41.8462,  -47.9389,  -54.0449,  -60.1622,  -66.2864,  -72.4141,
           -78.5488,  -84.8985,  -91.2473,  -97.5941, -103.9430, -110.2928,
          -116.6418, -122.9884, -129.3369, -135.6910, -142.0444, -148.3953,
          -154.7482,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [ -13.0037,  -18.1121,  -23.6261,  -29.3153,  -35.1134,  -40.9809,
           -46.8956,  -52.8408,  -58.8137,  -64.8065,  -70.8164,  -76.8367,
           -82.8715,  -88.9348,  -95.0165, -101.1090, -107.2137, -113.3269,
          -119.4456, -125.5661, -131.6927, -137.8257, -143.9607, -150.0947,
          -156.2382, -162.3795,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [ -19.2347,  -24.0545,  -29.3435,  -34.8496,  -40.4904,  -46.2242,
           -52.0219,  -57.8634,  -63.7467,  -69.6600,  -75.5961,  -81.5483,
           -87.5217,  -93.5137,  -99.5219, -105.5417, -111.5774, -117.6265,
          -123.6850, -129.7487, -135.8229, -141.9075, -147.9975, -154.0892,
          -160.1887, -166.3939, -172.5988, -178.8005, -185.0047, -191.2111,
          -197.4173,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [ -25.4366,  -30.0067,  -35.0852,  -40.4120,  -45.8952,  -51.4842,
           -57.1491,  -62.8690,  -68.6380,  -74.4437,  -80.2793,  -86.1367,
           -92.0187,  -97.9215, -103.8412, -109.7730, -115.7216, -121.6828,
          -127.6552, -133.6339, -139.6246, -145.6251, -151.6333, -157.6443,
          -163.6638, -169.6918, -175.7294, -181.7705, -187.8204, -193.8766,
          -199.9632, -206.0460, -212.1316, -218.2143, -224.2979, -230.3779,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [ -32.1606,  -36.7339,  -41.8136,  -47.1419,  -52.6116,  -58.2037,
           -63.8698,  -69.5919,  -75.3585,  -81.1674,  -87.0041,  -92.8631,
           -98.7317, -104.6377, -110.0525, -115.7458, -121.5559, -127.4269,
          -133.3347, -139.2656, -145.2120, -151.1797, -157.1600, -163.1481,
          -169.1476, -175.1534, -181.1679, -187.1882, -193.2159, -199.2544,
          -205.3051, -211.3622, -217.4288, -223.4972, -229.5690, -235.9026,
          -242.2390, -248.5777, -254.9155, -261.2508, -267.5886, -273.9324,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf, -111.1063, -116.5219, -122.2114, -128.0181, -133.8862,
          -139.7950, -145.7220, -151.6896, -157.6545, -163.6358, -169.6201,
          -175.6162, -180.9915, -186.6581, -192.4402, -198.2934, -204.1907,
          -210.1198, -216.0688, -222.0404, -228.0235, -234.0196, -240.0511,
          -246.1111, -252.1904, -258.2833, -264.3827, -270.4924, -276.6347,
          -282.7775, -288.9172, -295.0596, -301.2071, -307.3553, -313.5002,
          -319.6473],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
          -181.7981, -187.1755, -192.3928, -197.9278, -203.6075, -209.3755,
          -215.2007, -221.0621, -226.9585, -232.8793, -238.8198, -244.7792,
          -250.7644, -256.7698, -262.7921, -268.8254, -274.8752, -280.9459,
          -287.0273, -293.1139, -299.2108, -305.3179, -311.4301, -317.5430,
          -323.7829],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf, -193.0679, -198.2901, -203.8219, -209.5072, -215.2721,
          -221.1023, -226.4422, -232.0804, -237.8350, -243.6597, -249.5256,
          -255.4283, -261.3594, -267.3157, -273.2877, -279.2820, -285.2964,
          -291.3257, -297.3632, -303.4138, -309.4764, -315.5486, -321.6243,
          -327.8795],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
          -227.3243, -232.6623, -238.3008, -244.0516, -249.8768, -255.7409,
          -261.0763, -266.7046, -272.4574, -278.2776, -284.1507, -290.0634,
          -296.0017, -301.9580, -307.9352, -313.9305, -319.9380, -325.9536,
          -332.1356],
         [     -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf,      -inf,
               -inf,      -inf,      -inf,      -inf,      -inf, -261.9233,
          -267.2597, -272.8819, -278.6358, -284.4529, -290.3436, -296.2502,
          -301.6283, -307.2917, -313.0849, -318.9457, -324.8493, -330.7795,
          -337.0060]]], device='cuda:0')
ranges=tensor([[[ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 1,  2,  3,  4,  5],
         [ 2,  3,  4,  5,  6],
         [ 3,  4,  5,  6,  7],
         [ 3,  4,  5,  6,  7],
         [ 3,  4,  5,  6,  7],
         [ 3,  4,  5,  6,  7],
         [ 3,  4,  5,  6,  7],
         [ 4,  5,  6,  7,  8],
         [ 4,  5,  6,  7,  8],
         [ 4,  5,  6,  7,  8],
         [ 4,  5,  6,  7,  8],
         [ 4,  5,  6,  7,  8],
         [ 5,  6,  7,  8,  9],
         [ 5,  6,  7,  8,  9],
         [ 5,  6,  7,  8,  9],
         [ 5,  6,  7,  8,  9],
         [ 5,  6,  7,  8,  9],
         [ 5,  6,  7,  8,  9],
         [ 6,  7,  8,  9, 10],
         [ 6,  7,  8,  9, 10],
         [ 6,  7,  8,  9, 10],
         [ 6,  7,  8,  9, 10],
         [ 6,  7,  8,  9, 10],
         [ 6,  7,  8,  9, 10],
         [ 6,  7,  8,  9, 10]]], device='cuda:0')
ranges.shape=torch.Size([1, 48, 5])
teacher_logits.shape=torch.Size([1, 48, 5, 500])
model.simple_am_proj=Linear(in_features=384, out_features=500, bias=True)
params2={'frame_shift_ms': 10.0, 'allowed_excess_duration_ratio': 0.1, 'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 50, 'reset_interval': 200, 'valid_interval': 3000, 'feature_dim': 80, 'subsampling_factor': 4, 'warm_step': 2000, 'env_info': {'k2-version': '1.23.4', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': '2700d9558611e33b1da13608121e23a49bc69701', 'k2-git-date': 'Mon May 22 10:00:23 2023', 'lhotse-version': '1.13.0', 'torch-version': '1.12.1+cu113', 'torch-cuda-available': True, 'torch-cuda-version': '11.3', 'python-version': '3.1', 'icefall-git-branch': 'pruned-distillation', 'icefall-git-sha1': '9942098-dirty', 'icefall-git-date': 'Tue Feb 20 12:15:37 2024', 'icefall-path': '/home/sskim/miniconda3/lib/python3.10/site-packages/icefall-1.0-py3.10.egg', 'k2-path': '/home/sskim/miniconda3/lib/python3.10/site-packages/k2-1.23.4.dev20240124+cuda11.3.torch1.12.1-py3.10-linux-x86_64.egg/k2/__init__.py', 'lhotse-path': '/home/sskim/miniconda3/lib/python3.10/site-packages/lhotse/__init__.py', 'hostname': 'docker', 'IP address': '172.17.0.2'}, 'vocab_size': 500, 'blank_id': 0, 'context_size': 2, 'num_encoder_layers': '1,2,2,1,2', 'feedforward_dims': '256,256,512,512,256', 'nhead': '4,4,4,4,4', 'encoder_dims': '256,256,256,256,256', 'attention_dims': '192,192,192,192,192', 'encoder_unmasked_dims': '256,256,256,256,256', 'zipformer_downsampling_factors': '1,2,4,8,2', 'cnn_module_kernels': '31,31,31,31,31', 'decoder_dim': 512, 'joiner_dim': 512}
device=device(type='cuda', index=0)
Number of student model parameters: 11891797
ret['simple_loss']=tensor(345.2162, device='cuda:0', grad_fn=<NegBackward0>)
ret['pruned_loss']=tensor(325.8412, device='cuda:0', grad_fn=<SumBackward0>)
ret['pkd_loss']=tensor(16.0821, device='cuda:0', grad_fn=<DivBackward0>)
ret['ctc_loss']=tensor(25.5237, device='cuda:0', grad_fn=<MeanBackward0>)
****************************************************************************************************
Test ctc alignment
params={'frame_shift_ms': 10.0, 'allowed_excess_duration_ratio': 0.1, 'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 50, 'reset_interval': 200, 'valid_interval': 3000, 'feature_dim': 80, 'subsampling_factor': 4, 'warm_step': 2000, 'env_info': {'k2-version': '1.23.4', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': '2700d9558611e33b1da13608121e23a49bc69701', 'k2-git-date': 'Mon May 22 10:00:23 2023', 'lhotse-version': '1.13.0', 'torch-version': '1.12.1+cu113', 'torch-cuda-available': True, 'torch-cuda-version': '11.3', 'python-version': '3.1', 'icefall-git-branch': 'pruned-distillation', 'icefall-git-sha1': '9942098-dirty', 'icefall-git-date': 'Tue Feb 20 12:15:37 2024', 'icefall-path': '/home/sskim/miniconda3/lib/python3.10/site-packages/icefall-1.0-py3.10.egg', 'k2-path': '/home/sskim/miniconda3/lib/python3.10/site-packages/k2-1.23.4.dev20240124+cuda11.3.torch1.12.1-py3.10-linux-x86_64.egg/k2/__init__.py', 'lhotse-path': '/home/sskim/miniconda3/lib/python3.10/site-packages/lhotse/__init__.py', 'hostname': 'docker', 'IP address': '172.17.0.2'}, 'vocab_size': 500, 'blank_id': 0, 'context_size': 2, 'num_encoder_layers': '2,4,3,2,4', 'feedforward_dims': '1024,1024,2048,2048,1024', 'nhead': '8,8,8,8,8', 'encoder_dims': '384,384,384,384,384', 'attention_dims': '192,192,192,192,192', 'encoder_unmasked_dims': '256,256,256,256,256', 'zipformer_downsampling_factors': '1,2,4,8,2', 'cnn_module_kernels': '31,31,31,31,31', 'decoder_dim': 512, 'joiner_dim': 512, 'use_ctc': True}
device=device(type='cuda', index=0)
Number of model parameters: 70561891
VENICE
0: I DO NOT KNOW I AM DAZED BEWILDERED
1: HE STARTED AT THE THOUGHT HE HURRIED FORTH SADLY
2: THE CRAMPNESS AND THE POVERTY ARE ALL INTENDED
3: I UNDERSTAND BARTLEY I WAS WRONG
4: SOME OTHERS TOO BIG COTTON COUNTY
5: ALEXANDER WENT OVER AND OPENED THE WINDOW FOR HER
6: SHE SENT ME THE PAGES IN QUESTION BEFORE SHE DIED
7: CHAPTER ONE ORIGIN
8: CONSEIL WAS MY MANSERVANT
9: NEVER THAT SIR HE HAD SAID
10: FINE GLORIOUS
11: PAUL STICKS TO HIS THEME
12: VENICE
feature=tensor([[[-15.9375, -15.9502, -15.0228,  ..., -13.5457, -13.2630, -13.1002],
         [-15.9377, -15.5883, -13.9485,  ..., -11.7377, -12.9709, -13.2289],
         [-15.9380, -15.9467, -14.4766,  ..., -12.4176, -12.8154, -12.8583],
         ...,
         [-23.0259, -23.0259, -23.0259,  ..., -23.0259, -23.0259, -23.0259],
         [-23.0259, -23.0259, -23.0259,  ..., -23.0259, -23.0259, -23.0259],
         [-23.0259, -23.0259, -23.0259,  ..., -23.0259, -23.0259, -23.0259]]],
       device='cuda:0')
feature_lens=tensor([373], device='cuda:0')
feature.shape=torch.Size([1, 373, 80])
feature_lens.shape=torch.Size([1])
tmp_predicted_ids=tensor([], dtype=torch.int64)
hyp=''
y=RaggedTensor([[34, 75, 13, 93, 11]], dtype=torch.int32)
len(y.tolist()[0])=5
encoder_out_lens=tensor([92], device='cuda:0')
predids=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
       device='cuda:0')
predids.shape=torch.Size([1, 92])
supervisions['text'][iidx]='VENICE'
Point(token_index=0, time_index=15, score=0.027998751029372215)
Point(token_index=1, time_index=16, score=0.01935269869863987)
Point(token_index=1, time_index=17, score=0.875416100025177)
Point(token_index=2, time_index=18, score=0.1119103655219078)
Point(token_index=2, time_index=19, score=0.8022215962409973)
Point(token_index=2, time_index=20, score=0.8821399807929993)
Point(token_index=2, time_index=21, score=0.8712111115455627)
Point(token_index=3, time_index=22, score=0.0029585491865873337)
Point(token_index=4, time_index=23, score=0.013845914043486118)
fpath=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
len(fpath)=92
T: 92, U: 5
mpath=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 3, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
len(mpath)=92
****************************************************************************************************
Test using alphas
feature.shape=torch.Size([3, 1328, 80])
feature_lens.shape=torch.Size([3])
y=[[427, 33, 4, 193, 18, 89, 160, 9, 4, 54, 28, 81, 36, 275, 15, 61, 85, 162, 8, 367, 4, 287, 177, 11, 18, 256, 9, 51, 26, 136, 7, 223, 88, 165, 445, 3, 159, 34, 81, 53, 61, 37, 454, 141, 12, 16, 7, 51, 103, 24, 12, 63, 191, 7, 272, 50, 40, 210, 15, 4, 51, 26, 136, 5, 103, 88, 7, 15, 4, 223, 88, 5, 103, 88], [107, 33, 339, 11, 42, 4, 303, 6, 415, 362, 84, 5, 222, 142, 4, 111, 3, 96, 9, 4, 357, 256, 33, 137, 228, 93, 27, 96, 42, 4, 39, 11, 44, 41, 23, 146, 9, 5, 288, 53, 225, 374, 253, 9, 4, 254, 124, 17, 40, 36, 16, 42, 17, 48, 85, 256, 110, 169, 226, 63, 149, 267, 31, 49, 299, 23, 8], [142, 107, 62, 186, 5, 18, 18, 204, 21, 25, 205, 8, 4, 141, 24, 103, 43, 24, 24, 185, 143, 8, 255, 10, 56, 138, 10, 203, 18, 24, 123, 427, 4, 64, 89, 258, 171, 58, 8, 369, 189, 17, 14, 31, 9, 30, 3, 346, 5, 18, 18, 43, 12, 7, 22, 33, 4, 213, 59, 28, 234, 234, 25, 8, 433, 30, 62, 318, 339, 11, 321, 5, 119, 14, 296]]
len(y)=3
feature.shape=torch.Size([3, 1328, 80])
feature_lens.shape=torch.Size([3])
y.shape=[ [ x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x ] [ x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x ] [ x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x ] ]
####################################################################################################
To check ragens and logits
ranges=tensor([[[ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         ...,
         [70, 71, 72, 73, 74],
         [70, 71, 72, 73, 74],
         [70, 71, 72, 73, 74]],

        [[ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         ...,
         [63, 64, 65, 66, 67],
         [63, 64, 65, 66, 67],
         [63, 64, 65, 66, 67]],

        [[ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         [ 0,  1,  2,  3,  4],
         ...,
         [71, 72, 73, 74, 75],
         [71, 72, 73, 74, 75],
         [71, 72, 73, 74, 75]]], device='cuda:0')
logits=tensor([[[[-20.0564, -52.0548, -52.0548,  ..., -35.5792, -35.0926, -37.3797],
          [-18.3017, -51.1180, -51.1180,  ..., -34.8835, -35.7912, -36.8257],
          [-14.4097, -47.7075, -47.7075,  ..., -29.7753, -33.2196, -32.3745],
          [-18.2715, -46.8600, -46.8600,  ..., -30.1947, -32.9235, -30.6240],
          [-13.4856, -40.9381, -40.9381,  ..., -25.0183, -25.9355, -24.7068]],

         [[-19.1384, -52.0133, -52.0133,  ..., -35.5717, -33.9098, -37.3731],
          [-17.9760, -51.3255, -51.3255,  ..., -35.0014, -35.1209, -37.1123],
          [-13.9605, -48.0261, -48.0261,  ..., -29.6221, -32.5127, -32.8281],
          [-18.8311, -46.9675, -46.9675,  ..., -30.0541, -32.0844, -30.7391],
          [-13.0962, -40.9645, -40.9645,  ..., -24.6775, -25.2944, -24.5800]],

         [[-20.4504, -52.9757, -52.9757,  ..., -37.0350, -35.1595, -38.3369],
          [-18.7994, -52.4870, -52.4870,  ..., -36.5589, -36.5886, -37.9770],
          [-15.5061, -49.1595, -49.1595,  ..., -31.0891, -33.7732, -34.2053],
          [-19.4381, -47.5088, -47.5088,  ..., -31.2034, -32.9144, -31.2887],
          [-13.4787, -42.0721, -42.0721,  ..., -25.9492, -26.0696, -25.8441]],

         ...,

         [[-20.0591, -49.9168, -49.9168,  ..., -36.4453, -36.0628, -37.7652],
          [-18.0592, -51.2798, -51.2798,  ..., -38.0636, -37.6228, -40.9841],
          [-19.0464, -48.4616, -48.4616,  ..., -31.3584, -35.1251, -34.6746],
          [-14.7738, -45.3515, -45.3515,  ..., -33.2268, -31.5257, -30.5913],
          [-19.4869, -50.5215, -50.5215,  ..., -39.8698, -38.2225, -40.9062]],

         [[-21.4021, -49.7132, -49.7132,  ..., -36.4728, -36.6433, -37.3988],
          [-18.6131, -51.4695, -51.4695,  ..., -38.0420, -38.5942, -41.0606],
          [-20.1627, -50.0110, -50.0110,  ..., -32.6413, -37.4193, -35.9494],
          [-15.9084, -45.3662, -45.3662,  ..., -33.2993, -32.4134, -30.6660],
          [-20.1502, -50.5045, -50.5045,  ..., -39.7416, -39.0927, -41.1418]],

         [[-19.9586, -49.3220, -49.3220,  ..., -35.6717, -35.9099, -37.2671],
          [-18.2667, -50.6603, -50.6603,  ..., -37.0292, -37.6237, -40.4480],
          [-18.7266, -49.1936, -49.1936,  ..., -31.6200, -36.1788, -35.4401],
          [-14.8549, -45.0940, -45.0940,  ..., -32.5449, -31.3207, -30.7228],
          [-19.8866, -49.8210, -49.8210,  ..., -38.8601, -38.1142, -40.4982]]],


        [[[-18.2401, -50.0683, -50.0683,  ..., -35.8548, -33.9169, -36.2489],
          [-17.1560, -48.8615, -48.8615,  ..., -32.8892, -34.3617, -34.4962],
          [-16.2547, -49.1054, -49.1054,  ..., -34.1973, -35.6361, -34.5204],
          [ -7.7335, -38.9244, -38.9244,  ..., -22.5039, -23.9950, -25.5388],
          [-10.7394, -42.8567, -42.8567,  ..., -27.3211, -27.8472, -29.3280]],

         [[-19.8278, -50.5130, -50.5130,  ..., -36.3186, -35.0947, -36.4751],
          [-18.1889, -49.3798, -49.3798,  ..., -33.3209, -35.4415, -34.7704],
          [-17.6042, -49.7938, -49.7938,  ..., -34.9275, -36.6596, -35.0511],
          [ -7.1601, -37.9708, -37.9708,  ..., -22.3634, -23.7735, -24.5717],
          [-12.4984, -43.6175, -43.6175,  ..., -27.9681, -29.0315, -29.7411]],

         [[-17.4280, -49.9419, -49.9419,  ..., -35.7595, -34.0233, -35.9754],
          [-16.3251, -48.9764, -48.9764,  ..., -33.0370, -34.7193, -34.5047],
          [-15.3700, -48.9429, -48.9429,  ..., -34.1660, -35.6609, -34.4219],
          [ -6.5209, -37.8761, -37.8761,  ..., -22.0669, -23.2487, -24.3979],
          [-10.1249, -42.2038, -42.2038,  ..., -26.7013, -27.4368, -28.6795]],

         ...,

         [[-20.4422, -49.7642, -49.7642,  ..., -36.0111, -35.2956, -38.2320],
          [-13.8552, -43.8519, -43.8519,  ..., -31.1450, -30.5105, -28.3163],
          [ -9.3938, -40.6510, -40.6510,  ..., -26.6338, -24.1973, -25.1586],
          [ -9.5435, -39.2355, -39.2355,  ..., -24.3616, -24.6309, -22.8425],
          [-19.4797, -50.4589, -50.4589,  ..., -37.7183, -34.8532, -35.2224]],

         [[-20.8939, -49.9480, -49.9480,  ..., -36.1859, -35.1939, -38.1066],
          [-14.4587, -43.2581, -43.2581,  ..., -31.6435, -30.2162, -28.2857],
          [-10.0081, -39.8715, -39.8714,  ..., -26.4066, -24.1320, -24.9317],
          [ -9.3613, -39.3700, -39.3700,  ..., -24.5231, -24.6515, -22.8358],
          [-18.9474, -50.8073, -50.8073,  ..., -37.6507, -34.3939, -35.2869]],

         [[-19.7190, -49.0209, -49.0209,  ..., -35.2944, -34.4781, -37.6079],
          [-13.1337, -42.8879, -42.8879,  ..., -30.2022, -29.7757, -27.5104],
          [ -8.2719, -40.4768, -40.4768,  ..., -26.3792, -23.7334, -24.8672],
          [ -8.6237, -38.7373, -38.7373,  ..., -23.7979, -23.9518, -22.1389],
          [-18.8200, -49.9878, -49.9878,  ..., -37.3119, -34.1086, -34.6976]]],


        [[[-19.4089, -50.6667, -50.6667,  ..., -34.8012, -34.3946, -36.7240],
          [-17.7385, -50.7594, -50.7594,  ..., -33.7717, -35.9345, -36.6632],
          [-16.9598, -49.3748, -49.3748,  ..., -32.2974, -33.2718, -34.5395],
          [-17.9848, -47.6822, -47.6821,  ..., -31.4232, -33.7297, -35.9314],
          [-19.3837, -51.1783, -51.1783,  ..., -33.8824, -36.2956, -36.2707]],

         [[-18.7584, -50.1140, -50.1140,  ..., -33.9968, -33.0360, -36.0171],
          [-17.6906, -50.5458, -50.5458,  ..., -33.1475, -34.9860, -36.4214],
          [-16.2961, -48.5096, -48.5096,  ..., -31.1758, -31.7602, -33.6590],
          [-17.8847, -47.5076, -47.5076,  ..., -30.8569, -32.8454, -35.4277],
          [-19.4491, -50.7563, -50.7563,  ..., -33.2354, -35.4153, -35.8035]],

         [[-19.2893, -50.1811, -50.1811,  ..., -34.1715, -33.5246, -36.9556],
          [-17.7963, -50.5548, -50.5548,  ..., -33.4168, -35.5432, -37.2592],
          [-16.8271, -48.4825, -48.4825,  ..., -31.1734, -32.2549, -34.5626],
          [-18.0774, -47.5694, -47.5694,  ..., -31.0117, -33.2998, -36.3462],
          [-19.4072, -51.2724, -51.2724,  ..., -33.5460, -36.1888, -36.7785]],

         ...,

         [[-16.6578, -49.3119, -49.3119,  ..., -31.7091, -34.1813, -39.1645],
          [-14.5517, -44.4436, -44.4437,  ..., -29.1021, -31.2212, -30.3816],
          [ -9.8791, -37.9941, -37.9941,  ..., -22.8567, -24.1076, -25.0618],
          [-22.1346, -53.9104, -53.9104,  ..., -39.2315, -39.3075, -43.9832],
          [-19.5942, -50.0811, -50.0811,  ..., -33.4106, -35.5881, -42.4097]],

         [[-16.6299, -49.5827, -49.5827,  ..., -31.6236, -34.8287, -39.5625],
          [-14.5517, -44.1736, -44.1736,  ..., -28.5398, -30.8504, -30.1455],
          [ -8.6042, -37.8542, -37.8542,  ..., -22.3615, -24.0825, -24.7422],
          [-21.8716, -53.4249, -53.4249,  ..., -38.3526, -39.0562, -43.4345],
          [-19.8706, -49.9692, -49.9692,  ..., -32.8320, -35.5755, -42.1577]],

         [[-14.6878, -46.8353, -46.8353,  ..., -28.9522, -32.0680, -38.0063],
          [-12.5620, -41.5064, -41.5064,  ..., -26.1780, -28.2996, -28.2095],
          [ -5.9166, -35.1186, -35.1186,  ..., -19.9508, -21.5754, -23.1725],
          [-19.8148, -51.1927, -51.1927,  ..., -36.1800, -36.9376, -42.0643],
          [-17.9843, -47.4677, -47.4677,  ..., -30.3990, -32.9360, -40.7334]]]],
       device='cuda:0')
####################################################################################################
To get hypotheses from modified_beam_search
feature.shape=torch.Size([2, 1328, 80])
feature_lens.shape=torch.Size([2])
hyp_tokens[0]=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 254, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 454, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 367, 0, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 254, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 367, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 254, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
hyp_tokens[1]=[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 339, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 55, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 374, 0, 0, 0, 0, 0, 0, 0, 0, 0, 253, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 110, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 339, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 0, 0, 0, 5, 222, 0, 222, 0, 222, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 55, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 374, 0, 0, 0, 0, 0, 0, 0, 0, 0, 253, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 110, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 339, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 55, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 374, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 253, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 110, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 339, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 84, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 55, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 374, 0, 0, 0, 0, 0, 0, 0, 0, 0, 253, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 110, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
total_frame_count=330
diff_count00=0
diff_count01=1
diff_count02=3
diff_count03=1
